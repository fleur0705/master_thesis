{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YbdCKjTKMX3p",
        "96zZtb46MkEZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "YbdCKjTKMX3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minicons\n",
        "!pip install datasets\n",
        "!pip install mosestokenizer\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "VZER8wCOMdSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from minicons import scorer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from transformers import BertTokenizerFast, BertModel, BertForMaskedLM\n",
        "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
        "import math\n",
        "import pandas as pd\n",
        "from numpy import *\n",
        "from minicons.utils import character_span\n",
        "import os"
      ],
      "metadata": {
        "id": "b24lvJ7yMese"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function"
      ],
      "metadata": {
        "id": "96zZtb46MkEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  This function reads the CSV data from the specified URL and returns it as a pandas DataFrame\n",
        "'''\n",
        "def read_csv(csv_link):\n",
        "  url_data = csv_link.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "  return pd.read_csv(url_data, index_col=False)"
      ],
      "metadata": {
        "id": "U1Lp4b7AMnt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  This function extracts a list of lp\n",
        "\n",
        "  e.g.\n",
        "    lst_pair_token_lp = [('It', -1.5366992950439453), ('is', -0.263397216796875), ('the', -0.9581403732299805), ('vote', -9.064363479614258)]\n",
        "    lst_lp = [-1.5366992950439453, 0.263397216796875, 0.9581403732299805, -9.064363479614258]\n",
        "'''\n",
        "def extract_lst_lp(row):\n",
        "  lst_pair_token_lp = row['sentence_tokens_lp']\n",
        "  lst_lp = []\n",
        "  for i in range(len(lst_pair_token_lp)):\n",
        "    pair_token_lp = lst_pair_token_lp[i]\n",
        "    lst_lp.append(pair_token_lp[1])\n",
        "  return lst_lp\n",
        "  \n",
        "'''\n",
        "  This function computes lp for each token of each sentence\n",
        "  e.g.\n",
        "    Input: 'It is the vote'\n",
        "    Output: [('It', -1.5366992950439453), ('is', -0.263397216796875), ('the', -0.9581403732299805), ('vote', -9.064363479614258)]\n",
        "'''\n",
        "def compute_lst_pair_tokens_lp(row, model, language, model_type):\n",
        "  if (language == 'fr' and model_type == 'bert'):\n",
        "    return model.token_score(row['sentence'], surprisal = False)[0][1:]\n",
        "  else:\n",
        "    return model.token_score(row['sentence'], surprisal = False)[0]\n",
        "\n",
        "'''\n",
        "  This function computes surprisal for target token(s)\n",
        "'''\n",
        "def compute_surprisal_partiel(row, name, model, language, model_type):\n",
        "  sentence_token_lp = row['sentence_tokens_lp']\n",
        "\n",
        "  # extract lp for sentence\n",
        "  lst_lp_sentece = row['lst_lp']\n",
        "\n",
        "  # compute target lp\n",
        "  if (row[name] == 'na'):\n",
        "    return 0\n",
        "  target_token_lp = model.token_score(row[name], surprisal = False)[0]\n",
        "  if (language == 'fr' and model_type == 'bert'):\n",
        "    target_token_lp = target_token_lp[1:]\n",
        "  # extract targert token\n",
        "  target = [tup[0] for tup in target_token_lp]\n",
        "\n",
        "  # create a list of tuples with consecutive index numbers\n",
        "  index_token_sentence = list(enumerate([word[0] for word in sentence_token_lp]))\n",
        "  # initialize variables for the start and end index of the target span\n",
        "  start_index = None\n",
        "  end_index = None\n",
        "\n",
        "  # iterate through the index tuples to find the start and end indices of the target span\n",
        "  for i in range(len(index_token_sentence)):\n",
        "    # check if the current word matches the start of the target span\n",
        "    if index_token_sentence[i][1] == target[0]:\n",
        "      # check if the subsequent words match the rest of the target span\n",
        "      if (i+len(target) > len(index_token_sentence)):\n",
        "        return None\n",
        "      if [index_token_sentence[j][1] for j in range(i, i+len(target))] == target:\n",
        "        start_index = i\n",
        "        end_index = i+len(target)-1\n",
        "        break\n",
        "\n",
        "  # when no match\n",
        "  if (start_index == None or end_index == None):\n",
        "    target = row[name].split()\n",
        "    # if the length of target token is one\n",
        "    if (len(target)==1):\n",
        "      for i in range(len(index_token_sentence)):\n",
        "        if index_token_sentence[i][1] == target[0]:\n",
        "          return -lst_lp_sentece[i]\n",
        "        elif(target[0] == '#'):\n",
        "          return 0\n",
        "    else:\n",
        "      return None\n",
        "  # when it is well matched, return mean surprisal of targed tokens\n",
        "  else:\n",
        "    target_lp = lst_lp_sentece[start_index:end_index+1]\n",
        "    return -mean(target_lp)"
      ],
      "metadata": {
        "id": "VPaEuu0BMsSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(df, language, model_type, position, device, path_output_file):\n",
        "  # load language model\n",
        "  if (language == 'en'):\n",
        "    if (model_type == 'bert'):\n",
        "      model = scorer.MaskedLMScorer('bert-base-cased', device) \n",
        "    elif (model_type == 'gpt'):\n",
        "      model = scorer.IncrementalLMScorer('gpt2', device) \n",
        "  elif (language == 'fr'):\n",
        "    if (model_type == 'bert'):\n",
        "      model = scorer.MaskedLMScorer('flaubert/flaubert_base_cased', device)\n",
        "    elif (model_type == 'gpt'):\n",
        "      model = scorer.IncrementalLMScorer('asi/gpt-fr-cased-small', device)\n",
        "\n",
        "  # compute lp for each token of sentence \n",
        "  df['sentence_tokens_lp'] = df.apply(lambda x: compute_lst_pair_tokens_lp(x, model, language, model_type), axis=1)\n",
        "  # extract a list of tokens and of lp of token\n",
        "  df['lst_lp'] = df.apply(lambda x: extract_lst_lp(x), axis=1)\n",
        "  # compute mean lp for each sentence and store results in df\n",
        "  df['mean_lp'] = model.sequence_score(df['sentence'], reduction = lambda x: -x.mean(0).item())\n",
        "\n",
        "  # compute mean surprisal for each zone if considering 'zone'\n",
        "  if (position == 'zone'):\n",
        "    df['surprisal_zone1'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone1', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone2'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone2', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone3'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone3', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone4'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone4', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone5'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone5', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone6'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone6', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone7'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone7', model, language, model_type), axis=1)\n",
        "    df['surprisal_zone8'] = df.apply(lambda x: compute_surprisal_partiel(x, 'zone8', model, language, model_type), axis=1)\n",
        "\n",
        "  df.to_csv(path_output_file, index=False)"
      ],
      "metadata": {
        "id": "rLM2N_YEMxep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(path_file_dict, DIR, device='cuda'):\n",
        "  position = 'not zone'\n",
        "  for path_file in path_file_dict.keys():\n",
        "    df_exp = read_csv(path_file)\n",
        "    nb_exp = path_file_dict[path_file]\n",
        "    # The first three experiments are in English\n",
        "    if (nb_exp <4):\n",
        "      language = 'en'\n",
        "    # The rest of the experiment is in French\n",
        "    else:\n",
        "      language = 'fr'\n",
        "      # Experiments 7 and 8 compute the surprisal of each zone\n",
        "      if (nb_exp >=7):\n",
        "        position = 'zone'\n",
        "    # store all results\n",
        "    main(df_exp, language, 'bert', position, device, DIR+str(nb_exp)+'-bert.csv')\n",
        "    main(df_exp, language, 'gpt', position, device, DIR+str(nb_exp)+'-gpt.csv')"
      ],
      "metadata": {
        "id": "S2e8sBqDMz3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "MfBNrJsbNBd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = ''"
      ],
      "metadata": {
        "id": "Uw_f203cdlpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Part1 \n",
        "'''\n",
        "# Read materials\n",
        "df_en = read_csv('')\n",
        "df_fr = read_csv('')\n",
        "\n",
        "# Compute mean lp\n",
        "main(df_en_linear, 'en', 'bert', 'not zone', 'cuda', DIR+'gradient_en_bert.csv')\n",
        "main(df_en_linear, 'en', 'gpt', 'not zone', 'cuda', DIR+'gradient_en_gpt.csv')\n",
        "main(df_fr_linear, 'fr', 'bert', 'not zone', 'cuda', DIR+'gradient_fr_bert.csv')\n",
        "main(df_fr_linear, 'fr', 'gpt', 'not zone', 'cuda', DIR+'gradient_en_gpt.csv')"
      ],
      "metadata": {
        "id": "qxP7vCPENJh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Part2 and Part3\n",
        "'''\n",
        "# Define dict of materals\n",
        "path_file_dict =  {'':1, '':2, '':3, '':4, '':5, '':6, '':7, '':8}\n",
        "\n",
        "# Compute mean lp\n",
        "run(path_file_dict)"
      ],
      "metadata": {
        "id": "oXmmBwtFNHkD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}